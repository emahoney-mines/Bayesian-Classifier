<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>3383b37a989c4b31ac6f3d017f7c526c</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="csci-470-activities-and-case-studies" class="cell markdown"
id="5esGAjexJYJF">
<h2>CSCI 470 Activities and Case Studies</h2>
<ol>
<li>For all activities, you are allowed to collaborate with a
partner.</li>
<li>For case studies, you should work individually and are
<strong>not</strong> allowed to collaborate.</li>
</ol>
<p>By filling out this notebook and submitting it, you acknowledge that
you are aware of the above policies and are agreeing to comply with
them.</p>
</section>
<div class="cell markdown" id="vy84DmjMJYJG">
<p>Some considerations with regard to how these notebooks will be
graded:</p>
<ol>
<li>Cells in which "# YOUR CODE HERE" is found are the cells where your
graded code should be written.</li>
<li>In order to test out or debug your code you may also create notebook
cells or edit existing notebook cells other than "# YOUR CODE HERE". We
actually highly recommend you do so to gain a better understanding of
what is happening. However, during grading, <strong>these changes are
ignored</strong>.</li>
<li>You must ensure that all your code for the particular task is
available in the cells that say "# YOUR CODE HERE"</li>
<li>Every cell that says "# YOUR CODE HERE" is followed by a "raise
NotImplementedError". You need to remove that line. During grading, if
an error occurs then you will not receive points for your work in that
section.</li>
<li>If your code passes the "assert" statements, then no output will
result. If your code fails the "assert" statements, you will get an
"AssertionError". Getting an assertion error means you will not receive
points for that particular task.</li>
<li>If you edit the "assert" statements to make your code pass, they
will still fail when they are graded since the "assert" statements will
revert to the original. Make sure you don't edit the assert
statements.</li>
<li>We may sometimes have "hidden" tests for grading. This means that
passing the visible "assert" statements is not sufficient. The "assert"
statements are there as a guide but you need to make sure you understand
what you're required to do and ensure that you are doing it correctly.
Passing the visible tests is necessary but not sufficient to get the
grade for that cell.</li>
<li>When you are asked to define a function, make sure you
<strong>don't</strong> use any variables outside of the parameters
passed to the function. You can think of the parameters being passed to
the function as a hint. Make sure you're using all of those
variables.</li>
<li>Finally, <strong>make sure you run "Kernel &gt; Restart and Run
All"</strong> and pass all the asserts before submitting. If you don't
restart the kernel, there may be some code that you ran and deleted that
is still being used and that was why your asserts were passing.</li>
</ol>
</div>
<section id="case-study-bayesian-classifier" class="cell markdown"
id="6pYOHNtAJYJH">
<h1>Case Study: Bayesian classifier</h1>
<h2 id="part-1-a-bayesian-classifier-and-performance-evaluation">Part 1:
A Bayesian classifier and performance evaluation</h2>
<p>In this case study you will:</p>
<ol>
<li>Build, train, and test a Bayesian classifier. This will increase
your understanding of parametric models for supervised learning.</li>
<li>Implement the model as a Python class, in a manner similar to those
of scikit-learn. This will improve you understanding of what goes on
under the hood of scikit-learn models.</li>
<li>Evaluate performance of your Bayesian classifer, but also of some
"foolish" classifiers. This will alert you to the sometimes misleadingly
high performance scores a model might give, even when it has not
learning any relationship between features and targets.</li>
</ol>
<h3
id="in-this-part-1-notebook-you-will-focus-solely-on-the-bayesian-classifier-and-its-evaluation-in-the-part-2-notebook-you-will-build-the-foolish-classifiers-and-compare-evaluation-results-with-those-of-the-bayesian-classifier">In
this Part 1 notebook you will focus solely on the Bayesian classifier
and its evaluation. In the Part 2 notebook you will build the foolish
classifiers and compare evaluation results with those of the Bayesian
classifier.</h3>
</section>
<section id="the-bayesian-classifier" class="cell markdown"
id="bHrLknIuJYJH">
<h2>The Bayesian classifier</h2>
<p>In this course you have been introduced to one particular type of
Bayesian classifier--the Naive Bayes classifer. In that model, it is
assumed that all data features are independent of one another, thus
substantially reducing the complexity of the posterior probability
equation. In this notebook we will not assume independence but we will
assume that feature distributions are multivariate Gaussian with no
covariance between features--i.e., the covariance matrix is a diagonal
matrix. For a given class/label, just think of blobs of samples in
n-dimensional space, with different standard deviations ("widths") in
each dimension and no "diagonal" aspect to the blobs--like in the 2D
distribution in the image below, which has different standard deviations
in the vertical and horizontal directions, but no covariance between
them.</p>
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="./2d_noisy_gaussian.png"
alt="A 2D Gaussian distribution with diagonal covariance matrix, fitted to noisy data." /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A 2D Gaussian distribution with diagonal
covariance matrix, fitted to noisy data.</td>
</tr>
</tbody>
</table>
<p>Recall our Bayesian formulation of a supervised learning
classification problem. For class <span
class="math inline"><em>y</em></span> and feature vector <span
class="math inline"><strong>x</strong></span>,</p>
<p><span class="math display">$$P(y|\mathbf{x}) =
\frac{P(\mathbf{x}|y)P(y)}{P(\mathbf{x})}$$</span></p>
<p>For a given feature vector, <span
class="math inline"><strong>x</strong></span>, our goal is to find the
label <span class="math inline"><em>y</em></span> that gives the largest
value for <span
class="math inline"><em>P</em>(<em>y</em>|<strong>x</strong>)</span>.
Because <span class="math inline"><strong>x</strong></span> is a fixed
sample, <span class="math inline"><em>P</em>(<strong>x</strong>)</span>
is a constant, and this goal is equivalent to finding the label <span
class="math inline"><em>y</em></span> that maximizes <span
class="math inline"><em>P</em>(<strong>x</strong>|<em>y</em>)<em>P</em>(<em>y</em>)</span>.
Thus, we must use our training set to estimate <span
class="math inline"><em>P</em>(<strong>x</strong>|<em>y</em>)</span> and
<span class="math inline"><em>P</em>(<em>y</em>)</span>, and save that
information for when we use the model to make class predictions from
features.</p>
<p>We will assume that for a given class, <span
class="math inline"><em>y</em></span>, the distribution <span
class="math inline"><em>P</em>(<strong>x</strong>|<em>y</em>)</span> is
Gaussian, with covariance between features equal to 0. Furthermore, we
will work with data that has only two features, <span
class="math inline"><em>x</em><sub>1</sub></span> and <span
class="math inline"><em>x</em><sub>2</sub></span>, and can thus write
our probabilities as:</p>
<p><span class="math display">$$ P(x_1,x_2|y)=\frac{
exp\Big(-(\frac{{(x_1-\mu_{y,1})}^2}{\sigma_{y,1}^2} +
\frac{{(x_2-\mu_{y,2})}^2}{\sigma_{y,2}^2}) /2\Big)
}{2\pi\sigma_{y,1}\sigma_{y,2}}  $$</span></p>
<p>where <span
class="math inline"><em>μ</em><sub><em>y</em>, <em>k</em></sub></span>
and <span
class="math inline"><em>σ</em><sub><em>y</em>, <em>k</em></sub></span>
are the mean and standard deviation of the <span
class="math inline"><em>k</em></span>th feature for samples in class
<span class="math inline"><em>y</em></span>, respectively. Thus, our
final equation for our model's prediction, <span
class="math inline"><em>ỹ</em></span> is:</p>
<p><span class="math display">$$ \tilde{y} =
\underset{y}{\operatorname{argmax}} \Bigg[\frac{
exp\Big(-(\frac{{(x_1-\mu_{y,1})}^2}{\sigma_{y,1}^2} +
\frac{{(x_2-\mu_{y,2})}^2}{\sigma_{y,2}^2}) /2\Big)
}{2\pi\sigma_{y,1}\sigma_{y,2}} P(y) \Bigg] $$</span></p>
<p><strong>The above equation is your model, and you will use training
data to estimate the <span
class="math inline"><em>P</em>(<em>y</em>)</span> and the <span
class="math inline"><em>μ</em></span> and <span
class="math inline"><em>σ</em></span> parameters, and the equation
itself to make predictions."</strong> It may look complicated, but don't
worry, we'll guide you through the coding!</p>
</section>
<section id="the-models-python-class" class="cell markdown"
id="KyILNMgCJYJI">
<h2>The model's Python class</h2>
<p>You will create a Python class from scratch (aside from the skeleton
we have provided) that will implement three methods that are part of
most scikit-learn model classes:</p>
<p><code>fit(X, y)</code> - Trains the model using features X and label
y, and stores the learned (fitted) model parameters as class object
attributes.</p>
<p><code>predict(X)</code> - Using the stored model parameters, predicts
labels for features X, and returns them.</p>
<p><code>fit_predict(X, y)</code> - Sequentially performs the actions of
<code>fit()</code> and <code>predict()</code>, storing learned model
parameters and using those parameters to compute and return its
predictions <strong>for the same</strong> features, X, that are used
when fitting. Not all scikit-learn models implement this method. It is
often just a convenience function that literally calls
<code>fit()</code> and then <code>predict()</code>, but some
scikit-learn models may use a distinct implementation that is more
computationally efficient than sequential <code>fit()</code> and
<code>predict()</code> calls. Note that this is not a substitute for
splitting your data into train/val/test sets, as
<code>fit_predict()</code> does not do data splitting.</p>
<p>Before you start coding your model class,
<code>BayesClassifier</code>, keep in mind the manner in which it will
be used...</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A user instantiates the model class, indicating how many classes there</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># are in the initialization call.</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>bayes_clf <span class="op">=</span> BayesClassifer(n_classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The user then uses the class object to train on a subset of their data.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># In this case they&#39;ll obtain the predictions for the training set as well.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>y_pred_train <span class="op">=</span> bayes_clf.fit_predict(X_train, y_train)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># The user then makes predictions for the test set.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> bayes_clf.predict(X_test)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Finally, the user computes evaluation metric scores for y_pred_train</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># and y_pred_test (we&#39;ll discuss this later in the notebook).</span></span></code></pre></div>
<p>A skeleton of your code is in the cell below, along with guiding
comments. Note that typically a developer writes a lot of code that
ensures that the user is calling methods with necessary and sufficient
arguments, of the appropriate type and range. We will ignore that but
you are welcome to include such safechecks (e.g., <code>assert</code>
statements).</p>
<p>Write code for each of the methods of the class, implementing the
Bayesian classifier as described above, for data with 2 and only 2
features. Note that the number of classes is a variable specified by the
user--<strong>you can assume that labels are integers numbered from 0 to
N-1, for N classes.</strong> For example, if the user specifies that
<code>n_classes=4</code> then you can assume they will provide a numpy
array of labels that looks like
<code>y=np.array([2,3,1,1,0,2,3,3])</code>. Labels <strong>will
not</strong> be of some other format, such as "red", "blue",
"green".</p>
<p><strong>You may implement the methods using native/raw python and the
math and/or numpy modules. You may not use scikit-learn, however. Use of
numpy is suggested.</strong></p>
<p><code>numpy.where()</code>: As hinted in the skeleton code below,
during fitting, you'll need to find all the samples that belong to a
given class, and then do something with that subset of samples (e.g.,
compute the mean). One way of doing that is by using
<code>np.where()</code> to find the array indicies of the values that
meet that criterion, and then index your feature array with those
indices, to work on that subset of sample. The snippet of code below
demonstrates its usage.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>In [<span class="dv">1</span>]: <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>   ...: np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>   ...:                                                                                                                         </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>In [<span class="dv">2</span>]: <span class="co"># Create simple data  </span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>   ...: labels <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>])  </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>   ...: features <span class="op">=</span> np.random.randint(<span class="dv">0</span>, high<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="bu">len</span>(labels))  </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>   ...: <span class="bu">print</span>(features)  </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>   ...:                                                                                                                         </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>[<span class="dv">6</span> <span class="dv">3</span> <span class="dv">7</span> <span class="dv">4</span> <span class="dv">6</span> <span class="dv">9</span> <span class="dv">2</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>In [<span class="dv">3</span>]: <span class="co"># Find the indices of samples of class/label 1.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>   ...: <span class="co"># Note that np.where() returns a tuple of numpy arrays, even</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>   ...: <span class="co"># if the input to np.where() is only 1-dimensional. So you</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>   ...: <span class="co"># need to access the indices you want by extracting the</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>   ...: <span class="co"># 0th item of the tuple, using the results[0] syntax.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>   ...: idx <span class="op">=</span> np.where(labels<span class="op">==</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>   ...: <span class="bu">print</span>(idx)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>   ...: <span class="bu">print</span>(<span class="bu">len</span>(idx))  </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>   ...:                                                                                                                         </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span> <span class="dv">3</span> <span class="dv">4</span>]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>In [<span class="dv">4</span>]: <span class="co"># Compute the minimum feature value, for samples of class 1  </span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>   ...: <span class="bu">print</span>(features[idx])  </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>   ...: min_val <span class="op">=</span> np.<span class="bu">min</span>(features[idx])  </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>   ...: <span class="bu">print</span>(min_val)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>   ...:                                                                                                                         </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span> <span class="dv">4</span> <span class="dv">6</span>]</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>In [<span class="dv">5</span>]: <span class="co"># You could even do it all in one line..  </span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>   ...: min_val <span class="op">=</span> np.<span class="bu">min</span>(features[np.where(labels<span class="op">==</span><span class="dv">1</span>)[<span class="dv">0</span>]])  </span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>   ...: <span class="bu">print</span>(min_val)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>   ...:                                                                                                                         </span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span></span></code></pre></div>
<p>As a final reminder, you are implementing a type of Bayesian
classifier, but you are <strong>not</strong> implementing a Naive Bayes
classifier. A Gaussian Naive Bayes classifier would treat <span
class="math inline"><em>P</em>(<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>|<em>y</em>)</span>
as the product of two one-dimensional Gaussian instead: <span
class="math inline"><em>P</em>(<em>x</em><sub>1</sub>|<em>y</em>)<em>P</em>(<em>x</em><sub>2</sub>|<em>y</em>)</span>.
Such a distribution has more of a rectangular look versus the elliptical
look of the image in the cell above. You are welcome to examine
scikit-learn's source code for the Naive Bayes classifier (<a
href="https://github.com/scikit-learn/scikit-learn/blob/6c566da8c99b4908a549aeb659cd4b1124bc2448/sklearn/naive_bayes.py#L128">GaussianNB</a>),
but copying the code will not give identical results. If you are not an
advanced Python user, don't worry if the scikit-learn source code is
overwhelming. You're code can be much simpler, and you are probably
better off ignoring the scikit-learn code.</p>
</section>
<div class="cell code" id="lMcSMYY0JYJJ">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">&quot;ggplot&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" id="PK0FN7fUJYJK"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;ef12cbaa8f11a7c17d9496ca2dfcf38a&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;BayesClassifier&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Implement the three methods of your Bayesian classifier.</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">## YOU MAY NOT USE SCIKIT-LEARN IN THIS CELL.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesClassifier():</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the number of classes as a class attribute</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># You may initialize other variables here if you want to,</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># such as an (empty) array for storing feature means</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and variances, though it&#39;s not truly necessary.</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_classes <span class="op">=</span> n_classes</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> np.zeros(n_classes)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> np.zeros((n_classes, <span class="dv">2</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.variances <span class="op">=</span> np.zeros((n_classes, <span class="dv">2</span>))</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># raise NotImplementedError()</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, features, labels):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># features - a numpy array of shape (n_samples, n_features==2)</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels - a numpy array of shape (n_samples,)</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This method does not return/output anything.</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use X and y to fit your model parameters.</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For each of the 0 to self.n_classes-1 classes, you should:</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Determine which samples belong to that class, e.g., use np.where().</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Compute an estimate of the prior probability P(class), as the</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    number of samples of that class, divided by the total number of samples.</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    Store that value as an attribute, i.e., a variable that start with &quot;self.&quot;</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    Note that you&#39;ll likely want to store the prior probabilities in a</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    single list or array, of length n_classes.</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Compute the mean and variance for both features, for those samples.</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    You may use numpy.mean() and numpy.var()</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Store the means and variances as a class attribute, i.e., a</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    variable that start with &quot;self.&quot;. Again, you&#39;ll likely want to</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    stores the means in a list or array. Same for the variances. You will</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    have n_features*n_classes==2*n_classes means (and variances).</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># You may name your attribute variables anything that you wish, as allowed by python.</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># As a reminder, if you use the range() function to loop over</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the class labels [0, 1, 2, ...] recall that range() excludes the</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># value of the argument you give it. That is, range(4) will iterate</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># over 0, 1, 2, and 3. It will exclude 4.</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Count the number of samples in each class</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_classes):</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>            class_indices <span class="op">=</span> np.where(labels <span class="op">==</span> i)[<span class="dv">0</span>]</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>            class_features <span class="op">=</span> features[class_indices]</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.priors[i] <span class="op">=</span> <span class="bu">len</span>(class_indices) <span class="op">/</span> <span class="bu">len</span>(labels)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.means[i] <span class="op">=</span> np.mean(class_features, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.variances[i] <span class="op">=</span> np.var(class_features, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add a small diagonal to the covariance matrix if it is singular</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> np.linalg.det(np.diag(<span class="va">self</span>.variances[i])) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.variances[i] <span class="op">+=</span> np.eye(<span class="dv">2</span>) <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, features):</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># features - a numpy array of shape (n_samples, n_features==2)</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns an array of predictions</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use your fitted model parameters to make label predictions for</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the samples in &#39;features&#39;.</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. For each of the 0 to self.n_classes-1 classes, you should compute</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    P(X|y=class)*P(y=class), for all samples, and store those values.</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. For each/all samples, find the argmax across the n_class classes.</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    You may use numpy.argmax().</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Return those argmax values, which are the label predictions.</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>        posterior_probs <span class="op">=</span> np.zeros((<span class="bu">len</span>(features), <span class="va">self</span>.n_classes))</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_classes):</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>            likelihood_probs <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(((features <span class="op">-</span> <span class="va">self</span>.means[i]) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> <span class="va">self</span>.variances[i], axis<span class="op">=</span><span class="dv">1</span>)) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> np.sqrt(np.prod(<span class="va">self</span>.variances[i])))</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>            posterior_probs[:, i] <span class="op">=</span> likelihood_probs <span class="op">*</span> <span class="va">self</span>.priors[i]</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.argmax(posterior_probs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># raise NotImplementedError()</span></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit_predict(<span class="va">self</span>, features, labels):</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># features - a numpy array of shape (n_samples, n_features==2)</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels - a numpy array of shape (n_samples,)</span></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns an array of predictions</span></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit your model by calling self.fit(features, labels)</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Then make predictions by calling self.predict(features)</span></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Finally, return those predictions.</span></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fit(features, labels)</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.predict(features)</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>        <span class="co">#raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell markdown" id="mSa97AqZJYJL">
<p>The cell below does a coarse check of your implementation of the
BayesClassifier class. Unless you are a Python and numpy superstar, it
will likely fail on your first try. It may not give you any meaningful
feedback to help you debug your code.</p>
<p>To help with debugging, don't hesistate to:</p>
<ol>
<li>Put print statements in your code above, to show you the contents of
variables and ensure they hold the values you expect. Also print out
helpful info such as a variables type (e.g., are you sure it's a numpy
array? Or is it a list?... <code>print(type(my_variable))</code>, shape:
<code>print(my_variable.shape)</code>, and length:
<code>print(len(my_variable))</code>. Note that the output of the print
statements will appear below the cell in which you classifier is
instantiated, or one of its methods is called. It will not appear under
the cell in which the class is defined (above).</li>
<li>Create a new cell below, and create simple data you can use to test
your classifier (as in the autograder cell below).</li>
<li>If you instantiate your class, you can then view its attributes and
their values as shown below:</li>
</ol>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create object</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> BayesClassifier(n_classes<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print out the names of its attributes</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.__dict__.keys())</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># If an attribute is named &quot;means&quot; you can print out its values...</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.means)</span></code></pre></div>
<p>Keep in mind that depending on how you implemented your class, some
attributes may not exist until the <code>fit()</code> method is
called.</p>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="8K68p2gwJYJL"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;c8f084ba4110d0e5ca30ec2e10397e36&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;BayesClassifierTests&quot;,&quot;locked&quot;:true,&quot;points&quot;:30,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Create a few samples from 3 classes, with high class separation</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">5</span>], [<span class="dv">5</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">5</span>], [<span class="op">-</span><span class="dv">5</span>, <span class="op">-</span><span class="dv">2</span>]])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Test fit() and predict()</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> BayesClassifier(n_classes)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(y, y_pred)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Test fit_predict()</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.fit_predict(X, y)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(y, y_pred)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Try with 4 classes also</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">5</span>], [<span class="dv">5</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">5</span>], [<span class="op">-</span><span class="dv">5</span>, <span class="op">-</span><span class="dv">2</span>], [<span class="dv">10</span>, <span class="dv">11</span>], [<span class="dv">11</span>, <span class="dv">10</span>]])</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Test fit() and predict()</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> BayesClassifier(n_classes)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(y, y_pred)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Test fit_predict()</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.fit_predict(X, y)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(y, y_pred)</span></code></pre></div>
</div>
<section id="testing-and-visualization" class="cell markdown"
id="uqXRb9piJYJM">
<h2>Testing and visualization</h2>
<p>Now that you've completed all that hard work, let's try it out, and
visual some results. Since we are only using two features, we can easily
display results in 2D plots.</p>
<p>Below, we create the usual synthetic "blobs" of samples, with class
labels. We will train our model and then create a colormap that should
the classifiers boundaries between classes. In most real-world cases
your feature dimensionality will be too high to create such a colormap,
but we do it here for edification.</p>
</section>
<div class="cell code" id="rXsZzaHwJYJM">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random samples of three classes, with samples drawn from multivariate Gaussian distributions</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>n1 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>mean1 <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>cov1 <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>]]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>n2 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>mean2 <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">2</span>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>cov2 <span class="op">=</span> [[<span class="fl">0.1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>]]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>n3 <span class="op">=</span> <span class="dv">70</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>mean3 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">3</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>cov3 <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="fl">0.5</span>]]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [np.random.multivariate_normal(mean1, cov1, size<span class="op">=</span>n1),</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>     np.random.multivariate_normal(mean2, cov2, size<span class="op">=</span>n2),</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>     np.random.multivariate_normal(mean3, cov3, size<span class="op">=</span>n3),</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.concatenate(X, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>]<span class="op">*</span>n1 <span class="op">+</span> [<span class="dv">1</span>]<span class="op">*</span>n2 <span class="op">+</span> [<span class="dv">2</span>]<span class="op">*</span>n3)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:265}"
id="xqpUvkbPJYJM" data-outputId="1caaf374-3b31-48c1-e0d8-5452145da776">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let&#39;s train a model on the data set, then visualize the predictions,</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># as a subjective check. The predicted sample labels, plotted on the left, should</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># be color-coded much like the ground truth samples labels, plotted on the right,</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># except in regions were ground truth classes &quot;overlap.&quot;</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We&#39;re not yet concerned with evaluating a trained model&#39;s performance (on a</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># validation or test set), so we&#39;ll just use all our data for training.</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> BayesClassifier(n_classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.fit_predict(X, y)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot our synthetic data</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training data ground truth labels&quot;</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x1&quot;</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;x2&quot;</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">2</span>, <span class="dv">4</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>y_pred, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training data predicted labels&quot;</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x1&quot;</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;x2&quot;</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">2</span>, <span class="dv">4</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_4b7eda11ba6c4afaaf87cbcf3b296b64/f11b3f387fdc4ec784690d41bdcf2aef040202ff.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:368}"
id="iV8gIaQbJYJN" data-outputId="f1ddff81-aa68-4188-fb10-f7a720ff5389">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now lets create a map of the classification regions, and boundaries between them.</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># To do so we&#39;ll create a set of features (x1 and x2 values) that densely span the space,</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># get predictions for those features, and then plot the predicted labels as a colormap.</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">2</span>, <span class="dv">4</span>, <span class="fl">0.01</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>, <span class="fl">0.01</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>X1, X2 <span class="op">=</span> np.meshgrid(x1, x2)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>X_dense <span class="op">=</span> np.stack((X1.flatten(), X2.flatten()), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>y_dense <span class="op">=</span> clf.predict(X_dense)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>y_dense <span class="op">=</span> np.reshape(y_dense, (<span class="bu">len</span>(x1), <span class="bu">len</span>(x2)))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(y_dense, aspect<span class="op">=</span><span class="st">&#39;auto&#39;</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">False</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis()</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;x1&#39;</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;x2&#39;</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.title(<span class="st">&#39;Classifier decision boundaries&#39;</span>)</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_4b7eda11ba6c4afaaf87cbcf3b296b64/3e18367e17d9f06d30a08e7716c9833839f86320.png" /></p>
</div>
</div>
<section id="the-impact-of-the-class-prior-distribution"
class="cell markdown" id="epNZlMV6JYJN">
<h2>The impact of the class prior distribution</h2>
<p>If you wrote your classifier code correctly, all the images above
should look commensurate, and hopefully you now have a better
understanding of both Bayesian classifiers as well as how sckit-learn
models are implemented. Great! But before we move on to evaluation
metrics, let's do one more sanity check.</p>
<p>Remember that the Bayesian classifier takes into account the prior
distribution of classes--that is, the <span
class="math inline"><em>P</em>(<em>y</em>)</span> term in <span
class="math inline"><em>P</em>(<em>x̄</em>|<em>y</em>)<em>P</em>(<em>y</em>)</span>.
Imagine we have two classes with nearly the same feature distributions
(means and variances) but one of those classes has notably fewer samples
than the other class, i.e., it's prior is much lower. In that case, our
Bayesian classifier should nearly always select the class that is more
prevalent in the training set, since the <span
class="math inline"><em>P</em>(<em>x̄</em>|<em>y</em>)</span> terms are
about the same for both classes. Let's see what your classifier does in
that situation.</p>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:267}"
id="-oxX_nnMJYJN" data-outputId="9b5e2a84-6788-4039-e235-1083527166ad">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random samples of two classes with identical feature distributions,</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># but with many more samples of class 1 than class 2</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>XX <span class="op">=</span> [np.random.multivariate_normal([<span class="dv">0</span>, <span class="dv">0</span>], [[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>]], size<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>     np.random.multivariate_normal([<span class="dv">0</span>, <span class="dv">0</span>], [[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>]], size<span class="op">=</span><span class="dv">50</span>),</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>XX <span class="op">=</span> np.concatenate(XX, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>yy <span class="op">=</span> np.array([<span class="dv">0</span>]<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> [<span class="dv">1</span>]<span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Let train, predict, and plot, as before...</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> BayesClassifier(n_classes<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>yy_pred <span class="op">=</span> clf.fit_predict(XX, yy)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot our synthetic data</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>plt.scatter(XX[:, <span class="dv">0</span>], XX[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>yy, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training data ground truth labels&quot;</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x1&quot;</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;x2&quot;</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>plt.scatter(XX[:, <span class="dv">0</span>], XX[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>yy_pred, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training data predicted labels&quot;</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x1&quot;</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;x2&quot;</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_4b7eda11ba6c4afaaf87cbcf3b296b64/d792ec5f7129aa7e585b821e28761368d48148bf.png" /></p>
</div>
</div>
<div class="cell markdown" id="oHRyYn-lJYJO">
<p>If all is well, you should see that the classifier predicted nearly
all the samples to be of just one class--the class that had twice as
many samples in the training data.</p>
</div>
<section id="evaluation" class="cell markdown" id="1J7HzRefJYJO">
<h2>Evaluation</h2>
<p>Now, let's split our data into training and test sets, train the
model, and then evaluate it. In this section you'll assess and score the
model's test set predictions using tools we've discussed in the
course:</p>
<ul>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">confusion
matrix</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">accuracy</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html">precision,
recall, and F-score</a>.</li>
</ul>
<p><strong>We'll work with the 3-class synthetic data we initially
created in this notebook.</strong></p>
<p>You can use scikit-learn for performing the evaluation, but should
continue to use your BayesClassifier as the model class.</p>
</section>
<div class="cell code" id="G4ofXrBHJYJP">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split our data into train and test sets.</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that our data samples were ordered by class labels. We always want to randomly</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># shuffle the sample ordering before splitting data in to train/test sets. The</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># train_test_split() function does this for us, by default.</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, random_state<span class="op">=</span><span class="dv">0</span>, test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" id="pVbBo1VTJYJP"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;d60fc269b8119e00119985f91d29e292&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;Prediction&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In this cell you should do the following:</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Instantiate a new Bayesian classifier for a 3-class model, naming the classifier &#39;clf&#39;.</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Train your model on X_train and y_train and get predictions for the X_train features,</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">#    saving these to the variable &#39;y_pred_train&#39;.</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Use your model to make test set predictions for the X_test features, saving them</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">#    to the variable &#39;y_pred_test&#39;.</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate a new Bayesian classifier</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> BayesClassifier(n_classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>y_pred_train <span class="op">=</span> clf.fit_predict(X_train, y_train)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the model to make test set predictions for the X_test features</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co">#raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="b-yAYCQKJYJQ"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;91fb6f61eb0c0a0a253fb722f536a694&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;PredictionTest&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Check that the number of model&#39;s predictions are as expected</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">sum</span>(y_pred_test<span class="op">==</span>y_test)<span class="op">==</span><span class="dv">18</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">sum</span>(y_pred_train<span class="op">==</span>y_train)<span class="op">==</span><span class="dv">76</span></span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" id="56eSelekJYJQ"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;40cf8df7db7b2ba2b5823a0f9b64d50d&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;Evaluation&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In this cell you should use scikit-learn to get the confusion matrix,</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracy, precision, recall, and F-score. You should have two of each, one</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># for the training set and one for the test set.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Name your variables:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">#   cm_train, acc_train, prec_train, rec_train, fs_train</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   cm_test,  acc_test,  prec_test,  rec_test,  fs_test</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that precision_recall_fscore_support() returns four values. You do not</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># need the &#39;support&#39; value.</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, accuracy_score, precision_recall_fscore_support</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Training metrics</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>y_pred_train <span class="op">=</span> clf.fit_predict(X_train, y_train) <span class="co"># Train on the training set</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>cm_train <span class="op">=</span> confusion_matrix(y_train, y_pred_train)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>acc_train <span class="op">=</span> accuracy_score(y_train, y_pred_train)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>prec_train, rec_train, fs_train, _ <span class="op">=</span> precision_recall_fscore_support(y_train, y_pred_train)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Test metrics</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> clf.predict(X_test) <span class="co"># Get predictions for the test set</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>cm_test <span class="op">=</span> confusion_matrix(y_test, y_pred_test)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>acc_test <span class="op">=</span> accuracy_score(y_test, y_pred_test)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>prec_test, rec_test, fs_test, _ <span class="op">=</span> precision_recall_fscore_support(y_test, y_pred_test)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="co"># raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="Z7TJO1qpJYJQ"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;3229b53e7800cbeb8ca5515baecaf512&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;EvaluationTest&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">sum</span>(cm_test)<span class="op">==</span><span class="bu">len</span>(y_test)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">sum</span>(cm_train)<span class="op">==</span><span class="bu">len</span>(y_train)</span></code></pre></div>
</div>
<div class="cell markdown" id="RvoPkQb8JYJR">
<p>Let's print out the confusion tables and the scores. Take a look at
them. Do the seem in alignment with the plots we saw before, for the
model trained on all the data? If not, review your work.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="gIxy6uaiJYJR" data-outputId="2d9490a4-0402-44ae-bda6-32bcfdc23688">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Confusion table: train&#39;</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm_train)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Confusion table: test&#39;</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm_test)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Accuracies: train=</span><span class="sc">{</span>acc_train<span class="sc">}</span><span class="ss">, test=</span><span class="sc">{</span>acc_test<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Precisions:   train=</span><span class="sc">{</span>prec_train<span class="sc">}</span><span class="ss">, test=</span><span class="sc">{</span>prec_test<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Recalls:      train=</span><span class="sc">{</span>rec_train<span class="sc">}</span><span class="ss">, test=</span><span class="sc">{</span>rec_test<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;F-scores:     train=</span><span class="sc">{</span>fs_train<span class="sc">}</span><span class="ss">, test=</span><span class="sc">{</span>fs_test<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Confusion table: train
[[ 5  0  2]
 [ 0 15  0]
 [ 0  2 56]]

Confusion table: test
[[ 3  0  0]
 [ 0  5  0]
 [ 0  2 10]]

Accuracies: train=0.95, test=0.9

Precisions:   train=[1.         0.88235294 0.96551724], test=[1.         0.71428571 1.        ]
Recalls:      train=[0.71428571 1.         0.96551724], test=[1.         1.         0.83333333]
F-scores:     train=[0.83333333 0.9375     0.96551724], test=[1.         0.83333333 0.90909091]
</code></pre>
</div>
</div>
<section id="thats-it-for-part-1-in-part-2-you-will"
class="cell markdown" id="fQXrv10EJYJR">
<h2>That's it for Part 1! In Part 2 you will...</h2>
<ul>
<li>Build a couple "foolish" classifiers that base their predictions
only on some aspect of the class prior probabilites, ignoring the data
features altogether.</li>
<li>Write a function that computes one or more evaluation metrics using
numpy rather than scikit-learn.</li>
<li>Compute evaluation metrics for both the foolish classifiers and a
Bayesian classifier.</li>
<li>Compare results across classifiers, and discuss why the foolish
classifier(s) sometimes give misleadingly high scores.</li>
</ul>
</section>
<section id="feedback" class="cell markdown" data-deletable="false"
data-editable="false" id="PLuHo65WJYJS"
data-nbgrader="{&quot;cell_type&quot;:&quot;markdown&quot;,&quot;checksum&quot;:&quot;35c02446ada971330235203f8d3b177f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-d45e75d8071c765c&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false}">
<h2>Feedback</h2>
</section>
<div class="cell code" data-deletable="false" id="quigwb7oJYJS"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;cc89e0660ccb53529a249ada6cc1a0cd&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-fb93624c53422815&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true}">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedback():</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Provide feedback on the contents of this exercise</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">        string</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># N/A</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="v4TyFdA_JYJS"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;51c4d2a5734ab18322474a6f62fb5382&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-10ee4b2b9ba4be3d&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false}">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
</body>
</html>
